{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPt9CzbpjglyCuPjO5R4kdM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakshilade/DEEPLEARNING/blob/main/Sakshi_Lade_DeepLearningLabAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Name:** Sakshi Lade\n",
        "\n",
        "**Roll No.:**74\n",
        "\n",
        "**Batch:**DL4\n",
        "\n",
        "**Date Of Submission:**26/01/2025\n",
        "\n",
        "---\n",
        "\n",
        "#**Neural Network Implementation from Scratch**\n",
        "\n",
        "\n",
        "‚óè ***Objective:***\n",
        "\n",
        " Implement a simple feedforward neural network from scratch in Python without using\n",
        "any in-built deep learning libraries. This implementation will focus on basic components like\n",
        "forward pass, backward propagation (backpropagation), and training using gradient descent.\n",
        "\n",
        "---\n",
        "\n",
        "#**Problem Definition**\n",
        "\n",
        "\n",
        "***Dataset: XOR Dataset***\n",
        "- The dataset consists of **4 samples**, each with **2 input features** (X1, X2) and **1 output label** (Y).\n",
        "- **Structure of the XOR dataset:**\n",
        "\n",
        "| X1 | X2 | Y (Output) |\n",
        "|----|----|-----------|\n",
        "|  0  |  0  |  0  |\n",
        "|  0  |  1  |  1  |\n",
        "|  1  |  0  |  1  |\n",
        "|  1  |  1  |  0  |\n",
        "\n",
        "\n",
        "**XOR Logic Explanation:**\n",
        "  - The **output is 1** if **only one** of the inputs is **1**.\n",
        "  - If both inputs are **0 or both are 1**, the **output is 0**.\n",
        "  - This makes the dataset **non-linearly separable**, meaning a simple perceptron cannot solve it.\n",
        "\n",
        "\n",
        "  \n",
        "***Task:***   \n",
        "The task is a binary classification problem.  \n",
        "The neural network must learn to correctly classify XOR inputs into two categories: 0 or 1.  \n",
        "The output neuron represents the probability of the input belonging to class 1.\n",
        "\n",
        "---\n",
        "# **Methodology**\n",
        "\n",
        "***1. Neural Network Architecture***\n",
        "- The neural network consists of three layers:\n",
        "  - **Input Layer:** 2 neurons, representing the input features.\n",
        "  - **Hidden Layer:** 4 neurons, allowing the network to learn complex patterns.\n",
        "  - **Output Layer:** 1 neuron, producing the final prediction.\n",
        "- **Activation Functions Used:**\n",
        "  - **Hidden Layer:** Sigmoid activation function to introduce non-linearity.\n",
        "  - **Output Layer:** Sigmoid activation function for binary classification.\n",
        "\n",
        "---\n",
        "\n",
        " ***2. Forward Pass***   \n",
        "The forward pass is the process by which the input data is propagated through the network to generate an output.\n",
        "\n",
        "1. The input layer receives the data and passes it to the hidden layer.\n",
        "2. In the hidden layer:\n",
        "   - The input values are multiplied by weights and added to biases.\n",
        "   - The result is passed through the activation function (Sigmoid).\n",
        "3. The transformed values from the hidden layer are then passed to the output layer.\n",
        "4. The final output is computed using another set of weights, biases, and the Sigmoid function.\n",
        "\n",
        "---\n",
        "\n",
        "***3. Backpropagation***  \n",
        "Backpropagation is used to adjust the network's weights and biases to minimize the error.\n",
        "\n",
        "1. **Error Calculation:** The difference between the predicted and actual values is computed.\n",
        "2. **Gradient Computation:** The error is used to determine how much each weight contributed to the error.\n",
        "3. **Weight Adjustment:** The gradients are used to update the weights and biases in the direction that reduces error.\n",
        "4. **Iteration:** The process is repeated multiple times to improve accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "***4. Loss Function***\n",
        "\n",
        "\n",
        "- The **loss function** measures how well the neural network's predictions match the actual values.\n",
        "- In this case, since we are solving a binary classification problem, a common choice is **Mean Squared Error (MSE)**.\n",
        "- Alternatively, **Binary Cross-Entropy Loss** is often used for better performance.\n",
        "\n",
        "---\n",
        "\n",
        "***5. Optimization***\n",
        "- The **Gradient Descent Algorithm** is used to adjust the weights and minimize the loss.\n",
        "- The learning rate controls how much the weights are updated in each step:\n",
        "  - **A high learning rate** can cause the model to overshoot the optimal values.\n",
        "  - **A low learning rate** can slow down learning.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QX_xc2PkKtUR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiunM-1UKg9k",
        "outputId": "fe4eb17c-2d95-47ca-f934-d679f683cbb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Loss: 0.3628649445748759\n",
            "Epoch 1000 - Loss: 0.24498329076966135\n",
            "Epoch 2000 - Loss: 0.18685982538455695\n",
            "Epoch 3000 - Loss: 0.04307281066576964\n",
            "Epoch 4000 - Loss: 0.014284008722182549\n",
            "Epoch 5000 - Loss: 0.0075869856139063\n",
            "Epoch 6000 - Loss: 0.004958682297306169\n",
            "Epoch 7000 - Loss: 0.003614044341206018\n",
            "Epoch 8000 - Loss: 0.00281361718762735\n",
            "Epoch 9000 - Loss: 0.0022886602825666156\n",
            "\n",
            "Predictions after training:\n",
            "[[0.04779515]\n",
            " [0.95139372]\n",
            " [0.96211339]\n",
            " [0.03999602]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid Activation Function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of Sigmoid Activation Function\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Neural Network Class Definition\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Randomly initialize the weights and biases\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Weights and biases initialization with random values\n",
        "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)  # Weights from input to hidden layer\n",
        "        self.bias_hidden = np.random.randn(1, self.hidden_size)  # Bias for hidden layer\n",
        "\n",
        "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)  # Weights from hidden to output layer\n",
        "        self.bias_output = np.random.randn(1, self.output_size)  # Bias for output layer\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.input_layer = X  # Store the input data\n",
        "\n",
        "        # Calculate the input to the hidden layer and apply the activation function\n",
        "        self.hidden_layer_input = np.dot(self.input_layer, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_layer_output = sigmoid(self.hidden_layer_input)\n",
        "\n",
        "        # Calculate the input to the output layer and apply the activation function\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.output_layer_output = sigmoid(self.output_layer_input)\n",
        "\n",
        "        return self.output_layer_output\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        # Compute the error in the output layer\n",
        "        error_output = y - self.output_layer_output\n",
        "\n",
        "        # Calculate the gradient (delta) for the output layer\n",
        "        output_layer_delta = error_output * sigmoid_derivative(self.output_layer_output)\n",
        "\n",
        "        # Compute the error in the hidden layer\n",
        "        error_hidden = output_layer_delta.dot(self.weights_hidden_output.T)\n",
        "\n",
        "        # Calculate the gradient (delta) for the hidden layer\n",
        "        hidden_layer_delta = error_hidden * sigmoid_derivative(self.hidden_layer_output)\n",
        "\n",
        "        # Update weights from hidden to output layer\n",
        "        self.weights_hidden_output += self.hidden_layer_output.T.dot(output_layer_delta) * learning_rate\n",
        "\n",
        "        # Update bias for the output layer\n",
        "        self.bias_output += np.sum(output_layer_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "        # Update weights from input to hidden layer\n",
        "        self.weights_input_hidden += X.T.dot(hidden_layer_delta) * learning_rate\n",
        "\n",
        "        # Update bias for the hidden layer\n",
        "        self.bias_hidden += np.sum(hidden_layer_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            # Perform a forward pass\n",
        "            self.forward(X)\n",
        "\n",
        "            # Perform a backward pass (backpropagation)\n",
        "            self.backward(X, y, learning_rate)\n",
        "\n",
        "            # Print loss (mean squared error) every 1000 epochs\n",
        "            if epoch % 1000 == 0:\n",
        "                loss = np.mean(np.square(y - self.output_layer_output))  # Mean squared error\n",
        "                print(f\"Epoch {epoch} - Loss: {loss}\")\n",
        "\n",
        "# Main Program\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the XOR problem as a simple example\n",
        "    X = np.array([[1, 1], [0, 1], [1, 0], [0, 0]])  # Input data (XOR inputs)\n",
        "    y = np.array([[0], [1], [1], [0]])\n",
        "    nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "\n",
        "    # Train the network with the XOR dataset for 10,000 epochs and a learning rate of 0.1\n",
        "    nn.train(X, y, epochs=10000, learning_rate=0.1)\n",
        "\n",
        "    # After training, print the final predictions of the network\n",
        "    print(\"\\nPredictions after training:\")\n",
        "    print(nn.forward(X))  # Test the network on the XOR inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Declaration:**\n",
        "\n",
        "\n",
        "I, Sakshi Lade, confirm that the work submitted in this assignment is my own and has been completed\n",
        "following academic integrity guidelines. The code is uploaded on my GitHub repository account, and the\n",
        "repository link is provided below:  \n",
        "GitHub Repository Link:  \n",
        "Signature:Sakshi Lade"
      ],
      "metadata": {
        "id": "I5dYDWWFR6_r"
      }
    }
  ]
}